{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mzalqF8y8Wgl",
        "CBOktJsX6n8D",
        "prO6HuxLw7Wo",
        "bPEXp0uR-iVm",
        "xSrYj3ov-m6C",
        "XYsN9_QngbhY",
        "f-AQhuwsqDgU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering with Large Language Models (e.g. GPT)\n",
        "\n",
        "The goal of this exercise is to help you understand the concept of prompt engineering with large language models, such as GPT-3, and practice creating effective prompts for specific tasks or information retrieval.\n",
        "\n",
        "This exercise also incorporates retrieval augmented generation (RAG) techniques. You will practice creating effective prompts that involve retrieving information and generating coherent responses.\n",
        "\n",
        "For a deeper dive, checkout the [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/chapter0/1).\n"
      ],
      "metadata": {
        "id": "GvQESicA8GXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "**IMPORTANT: You need to run the Code in each section, in the order that it appears, each time you connect to a runtime.**\n",
        "\n",
        "To get started with this notebook, click \"File\", and \"Save a copy\" to a location of your choosing."
      ],
      "metadata": {
        "id": "_dJIvOjIwPT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "mzalqF8y8Wgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install langchain, which we'll use to run\n",
        "# interactions with LLMs\n",
        "!pip install --upgrade langchain\n",
        "\n",
        "# install the LLMs we're going to use\n",
        "!pip install gpt4all openai\n",
        "\n",
        "# install bs4 (BeautifulSoup) for web loading\n",
        "!pip install bs4\n",
        "\n",
        "# install pypdf for PDF reading\n",
        "!pip install pypdf\n",
        "\n",
        "# install the document embedding and indexing tools\n",
        "!pip install faiss-cpu huggingface-hub sentence_transformers\n",
        "\n",
        "# install other dependencies\n",
        "!pip install cohere tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jcysM2pwY8z",
        "outputId": "0fec0e6c-8403-4118-e280-f52fa4f23c87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.334-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.2-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.62 (from langchain)\n",
            "  Downloading langsmith-0.0.63-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.2 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.334 langsmith-0.0.63 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting gpt4all\n",
            "  Downloading gpt4all-2.0.2-py3-none-manylinux1_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-1.2.3-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt4all) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.66.1)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2.0.7)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, gpt4all, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gpt4all-2.0.2 h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.2.3\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1257 sha256=8c5afa6558a8b03227902a0f91ac7a83a67f5ab1a43a2c00cba094d4316f7298\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "Successfully built bs4\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.1\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.17.0-py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.4/277.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.17.0\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (23.2)\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2023.7.22)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=be7d282b275df97478279d8d1695732b3d551f9a8e906f24c152a47698cf9159\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: sentencepiece, faiss-cpu, safetensors, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
            "Successfully installed faiss-cpu-1.7.4 huggingface-hub-0.17.3 safetensors-0.4.0 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.35.0\n",
            "Collecting cohere\n",
            "  Downloading cohere-4.34-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.8.6)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro==1.8.2 (from cohere)\n",
            "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (6.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (2023.7.22)\n",
            "Installing collected packages: fastavro, backoff, tiktoken, cohere\n",
            "Successfully installed backoff-2.2.1 cohere-4.34 fastavro-1.8.2 tiktoken-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilities"
      ],
      "metadata": {
        "id": "CBOktJsX6n8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "\"\"\"\n",
        "UTILS\n",
        "\"\"\"\n",
        "\n",
        "def now_ms(get_time=time.time):\n",
        "    \"\"\"\n",
        "    Returns the current time in milliseconds.\n",
        "    \"\"\"\n",
        "    return round(get_time() * 1000)\n",
        "\n",
        "\n",
        "def duration_ms(start_time, now_ms=now_ms):\n",
        "    \"\"\"\n",
        "    Returns the duration in milliseconds since the given start time.\n",
        "    \"\"\"\n",
        "    return now_ms() - start_time\n",
        "\n",
        "\n",
        "def duration_s(start_time, now_ms=now_ms):\n",
        "    \"\"\"\n",
        "    Returns the duration in seconds since the given start time.\n",
        "    \"\"\"\n",
        "    return duration_ms(start_time, now_ms) / 1000\n",
        "\n",
        "\n",
        "SHOULD_TRACE = True\n",
        "START_TIME = now_ms()\n",
        "\n",
        "\n",
        "def trace(*args, **kwargs):\n",
        "    \"\"\"\n",
        "    Prints a trace message if the `SHOULD_TRACE` flag is set to True.\n",
        "    \"\"\"\n",
        "    if SHOULD_TRACE:\n",
        "        print(f\"TRACE::{duration_s(START_TIME)}::\", *args, **kwargs)\n"
      ],
      "metadata": {
        "id": "jUyRdZKsxGBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Index Loading"
      ],
      "metadata": {
        "id": "prO6HuxLw7Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain.embeddings import (GPT4AllEmbeddings, HuggingFaceEmbeddings,\n",
        "                                  OpenAIEmbeddings)\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "\n",
        "def use_huggingface_embeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepares the HuggingFace embeddings.\n",
        "    \"\"\"\n",
        "    return HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "\n",
        "def use_openai_embeddings():\n",
        "    \"\"\"\n",
        "    Prepares the OpenAI embeddings.\n",
        "    \"\"\"\n",
        "    return OpenAIEmbeddings()\n",
        "\n",
        "\n",
        "def use_gpt4all_embeddings():\n",
        "    \"\"\"\n",
        "    Prepares the GPT4All embeddings.\n",
        "    \"\"\"\n",
        "    return GPT4AllEmbeddings()\n",
        "\n",
        "\n",
        "def split_and_save_index(\n",
        "    documents,\n",
        "    store_path,\n",
        "    make_embeddings=use_gpt4all_embeddings,\n",
        "):\n",
        "    \"\"\"\n",
        "    Parses the loader output and stores it in a local vector store.\n",
        "\n",
        "    PARAMETERS:\n",
        "    documents (list):  The documents to parse.\n",
        "    store_path (str):  The path to the local vector store to\n",
        "                       save the parsed PDF to.\n",
        "    model_name (str):  The name of the HuggingFace model to use\n",
        "                       for generating embeddings.\n",
        "\n",
        "    RETURNS:\n",
        "    (FAISS): The loaded index.\n",
        "    \"\"\"\n",
        "    trace(\"splitting source\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1024, chunk_overlap=64)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    trace(\"preparing embeddings\")\n",
        "    embeddings = make_embeddings()\n",
        "    trace(\"indexing source\")\n",
        "    faiss_index = FAISS.from_documents(texts, embeddings)\n",
        "    faiss_index.save_local(store_path)\n",
        "    return faiss_index\n",
        "\n",
        "\n",
        "def load_index_from_cache(\n",
        "    store_path,\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Loads the embeddings and index from a local vector store.\n",
        "\n",
        "    PARAMETERS:\n",
        "    store_path (str):  The path to the local vector store to\n",
        "                       load the embeddings and index from.\n",
        "\n",
        "    RETURNS:\n",
        "    (FAISS): The loaded index.\n",
        "    \"\"\"\n",
        "    trace(\"preparing embeddings\")\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
        "    trace(\"loading index\")\n",
        "    faiss_index = FAISS.load_local(store_path, embeddings)\n",
        "    return faiss_index\n",
        "\n",
        "\n",
        "def load_index(index_path, make_loader):\n",
        "    \"\"\"\n",
        "    Loads the index from the given path. If the index doesn't exist,\n",
        "    it will be created and cached.\n",
        "\n",
        "    PARAMETERS:\n",
        "    index_path (str): The path to the index.\n",
        "    make_loader (function): A function that returns a document loader.\n",
        "\n",
        "    RETURNS:\n",
        "    (FAISS): The loaded index.\n",
        "    \"\"\"\n",
        "    path_exists = os.path.isdir(index_path)\n",
        "\n",
        "    if path_exists is False:\n",
        "        trace(f\"loading new index {index_path}\")\n",
        "        split_and_save_index(\n",
        "            make_loader().load_and_split(),\n",
        "            index_path,)\n",
        "\n",
        "    trace(f\"using cached index {index_path}\")\n",
        "    return load_index_from_cache(index_path)\n"
      ],
      "metadata": {
        "id": "cGqgECenxgeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLMs"
      ],
      "metadata": {
        "id": "vXfDS8yWy_PR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GPT4All\n",
        "\n",
        "[GPT4All](https://gpt4all.io/index.html) is a free-to-use, locally running, privacy-aware chatbot. No GPU or internet required.\n",
        "\n",
        "You can use GPT4All in the same way that you would chatGPT or OpenAI GPT APIs.\n",
        "\n",
        "This function produces a [chainable GPT4All LLM](https://python.langchain.com/docs/integrations/llms/gpt4all) that can be composed with [langchain LCEL chains](https://python.langchain.com/docs/expression_language)."
      ],
      "metadata": {
        "id": "bPEXp0uR-iVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All\n",
        "from langchain import llms\n",
        "\n",
        "def use_gpt4all(model_name, model_cache_path, fq_model_path):\n",
        "    \"\"\"\n",
        "    Prepares a LLM for use in Q&A. Downloads the model to the\n",
        "    cache if it isn't already there.\n",
        "\n",
        "    PARAMETERS:\n",
        "    model_name (str):       The name of the model to use.\n",
        "    model_cache_path (str): The path to the model cache.\n",
        "    fq_model_path (str):    The fully qualified path to the model.\n",
        "\n",
        "    RETURNS:\n",
        "    (Any (LCEL<LLM>)): The prepared LLM.\n",
        "    \"\"\"\n",
        "    trace(\"preparing LLM\")\n",
        "    trace(f\"model_name={model_name}\")\n",
        "    trace(f\"model_cache_path={model_cache_path}\")\n",
        "    trace(f\"fq_model_path={fq_model_path}\")\n",
        "\n",
        "    # if the chosen model isn't cached, this will\n",
        "    # load it into the cache for future use\n",
        "    # (~/.cache/gpt4all on linux & macos)\n",
        "    GPT4All(model_name=model_name, model_path=model_cache_path)\n",
        "\n",
        "    # load an LCEL chain wrapper of GPT4All\n",
        "    return llms.GPT4All(model=fq_model_path)\n"
      ],
      "metadata": {
        "id": "QkMrd3RTzC_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OpenAI\n",
        "\n",
        "[OpenAI](https://platform.openai.com/) is a pay-to-use, cloud running, exposed (not private) chatbot. No GPU is required, but you need an internet connection to use it.\n",
        "\n",
        "You will need an API key to use OpenAI, which you can create on the [API Keys](https://platform.openai.com/account/api-keys) page after setting up your billing account.\n",
        "\n",
        "This function produces a [chainable OpenAI LLM](https://python.langchain.com/docs/integrations/llms/gpt4all) that can be composed with [langchain LCEL chains](https://python.langchain.com/docs/expression_language)."
      ],
      "metadata": {
        "id": "xSrYj3ov-m6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "from langchain import llms\n",
        "\n",
        "def useOpenAI():\n",
        "    \"\"\"\n",
        "    Prepares the OpenAI LLM. Prompts for the API key if it isn't\n",
        "    already set in the environment.\n",
        "    \"\"\"\n",
        "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if OPENAI_API_KEY is None:\n",
        "        OPENAI_API_KEY = getpass(\n",
        "            \"Enter your OpenAI API key (from: https://platform.openai.com/account/api-keys)\")\n",
        "\n",
        "    return llms.OpenAI(openai_api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "NWMTmney-pDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choose your LLM\n",
        "\n",
        "Two options for LLMs are provided: GPT4All and OpenAI. GPT4All is free, runs locally, and is private. However it is much slower than OpenAI. If you want faster responses and have free tokens, or are willing to spend $6+, you can choose to use OpenAI.\n",
        "\n",
        "Once you choose which LLM you wish to use, make sure it is uncommented and that the other one is commented in the next code block, **and then run it**.\n",
        "\n",
        "From this point on, in the document, use `llm` whenever you chain the LLM.\n",
        "\n",
        "_NOTE If you choose GPT4All, this will take a while, It needs to download the model you choose for GPT4All. I chose a mini model by default to reduce the loading time_.\n",
        "\n",
        "If you choose GPT4All, you can choose our model and there are a lot of choices. Here are some of the most effective options (from https://gpt4all.io/index.html, 2023-11):\n",
        "\n",
        "<details>\n",
        "  <summary>Best overall fast chat model: `mistral-7b-openorca.Q4_0.gguf` (click to expand)</summary>\n",
        "\n",
        "- Fast responses\n",
        "- Chat based model\n",
        "- Trained by Mistral AI\n",
        "- Finetuned on OpenOrca dataset curated via Nomic Atlas\n",
        "- Licensed for commercial use\n",
        "- SIZE: 3.83 GB\n",
        "- RAM: 8 GB\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "  <summary>Best overall fast instruction following model: `mistral-7b-instruct-v0.1.Q4_0.gguf` (click to expand)</summary>\n",
        "\n",
        "- Fast responses\n",
        "- Trained by Mistral AI\n",
        "- Uncensored\n",
        "- Licensed for commercial use\n",
        "- SIZE: 3.83 GB\n",
        "- RAM: 8 GB\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "  <summary>Very fast model with good quality: `gpt4all-falcon-q4_0.gguf` (click to expand)</summary>\n",
        "\n",
        "- Fastest responses\n",
        "- Instruction based\n",
        "- Trained by TII\n",
        "- Finetuned by Nomic AI\n",
        "- Licensed for commercial use\n",
        "- SIZE: 3.92 GB\n",
        "- RAM: 8 GB\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "  <summary>Best overall larger model: `wizardlm-13b-v1.2.Q4_0.gguf` (click to expand)</summary>\n",
        "\n",
        "- Instruction based\n",
        "- Gives very long responses\n",
        "- Finetuned with only 1k of high-quality data\n",
        "- Trained by Microsoft and Peking University\n",
        "- Cannot be used commercially\n",
        "- SIZE: 6.86 GB\n",
        "- RAM: 16 GB\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "  <summary>Extremely good model: `nous-hermes-llama2-13b.Q4_0.gguf` (click to expand)</summary>\n",
        "\n",
        "- Instruction based\n",
        "- Gives long responses\n",
        "- Curated with 300,000 uncensored instructions\n",
        "- Trained by Nous Research\n",
        "- Cannot be used commercially\n",
        "- SIZE: 6.86 GB\n",
        "- RAM: 16 GB\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "  <summary>Very good overall model: `gpt4all-13b-snoozy-q4_0.gguf` (click to expand)</summary>\n",
        "\n",
        "- Instruction based\n",
        "- Based on the same dataset as Groovy\n",
        "- Slower than Groovy, with higher quality responses\n",
        "- Trained by Nomic AI\n",
        "- Cannot be used commercially\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "VD9vqKBXHQ3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_gpt4all():\n",
        "    GPT_MODEL_NAME = \"orca-mini-3b-gguf2-q4_0.gguf\"\n",
        "    GPT_MODEL_CACHE_PATH = \"models/\" # os.path.expanduser(\"~/.cache/gpt4all/\")\n",
        "    GPT_FQ_MODEL_PATH = f\"{GPT_MODEL_CACHE_PATH}{GPT_MODEL_NAME}\"\n",
        "\n",
        "    return use_gpt4all(GPT_MODEL_NAME, GPT_MODEL_CACHE_PATH, GPT_FQ_MODEL_PATH)\n",
        "\n",
        "# if you choose to use gpt4all, you need to add a \"models\" directory\n",
        "# in colab, click the folder icon on the left of the screen,\n",
        "# right click in the filesystem palette, choose \"New folder\",\n",
        "# and name it, \"models\"\n",
        "llm = choose_gpt4all()\n",
        "# llm = useOpenAI()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK1OAT_wJl-Z",
        "outputId": "f69ed11b-80e9-4e7f-b36e-d1ff8db78214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRACE::245.111:: preparing LLM\n",
            "TRACE::245.113:: model_name=orca-mini-3b-gguf2-q4_0.gguf\n",
            "TRACE::245.113:: model_cache_path=models/\n",
            "TRACE::245.113:: fq_model_path=models/orca-mini-3b-gguf2-q4_0.gguf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.98G/1.98G [01:06<00:00, 29.7MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading 1: Zero-shot, Few-shot Prompting, and Retrieval Augmented Generation (RAG)\n",
        "\n",
        "1. **Zero-shot prompting**:\n",
        "\n",
        "   Zero-shot prompting is a technique used to interact with large language models, like GPT-3, without providing any specific training data or examples. In this approach, users can simply describe their task or question, and the model can generate a response based on its pre-existing knowledge and understanding of language. Zero-shot prompting doesn't require prior data or examples related to the specific query. Instead, it relies on the model's general understanding of language and its ability to generate contextually appropriate responses even for unseen topics or tasks. This makes it a versatile tool for various natural language understanding and generation tasks.\n",
        "\n",
        "   **Example**: A user asks, \"Explain the theory of relativity.\" In this zero-shot scenario, the model generates a coherent explanation of Einstein's theory of relativity without any pre-defined training data.\n",
        "\n",
        "2. **Few-shot prompting**:\n",
        "\n",
        "   Few-shot prompting is an extension of zero-shot prompting where users provide a small number of examples or context to guide the model's response. Instead of relying solely on the model's general knowledge, users give a few pieces of information or examples related to the task to help the model understand the context better. This approach is useful when the user wants to fine-tune the model's response for a specific task or topic, and it allows the model to adapt more precisely to the user's requirements.\n",
        "\n",
        "   **Example**: A user asks, \"Translate the following English text into Spanish: 'The quick brown fox jumps over the lazy dog.' Here are a few sample translations for reference: 'Hello, world: Hola Mundo,' 'The sun is shining: El sol está brillando,' 'It's a beautiful day: Es un hermoso día.'\" The model uses these examples to generate a translation, even if it hasn't seen this specific input before.\n",
        "\n",
        "3. **Retrieval Augmented Generation (RAG)**:\n",
        "\n",
        "   Retrieval Augmented Generation (RAG) is a technique that combines the capabilities of large language models with information retrieval from external sources. It involves retrieving relevant information from a knowledge base or external documents and using that retrieved information to enhance the model's generated output. In RAG, the model first searches for and extracts information from a vast dataset or external sources, and then it generates a response that integrates this retrieved knowledge. This allows the model to provide more accurate, contextually rich, and factually grounded responses by leveraging external information.\n",
        "\n",
        "   **Example**: When asked a question about a historical event, the model can search a database of historical documents to retrieve information and then generate a response that incorporates those historical facts.\n",
        "\n",
        "In summary, zero-shot prompting relies on the model's pre-existing knowledge, few-shot prompting provides a limited amount of context or examples to fine-tune responses, and retrieval augmented generation involves an explicit retrieval step to gather information from external sources before generating a response. These techniques are valuable for a wide range of natural language processing tasks."
      ],
      "metadata": {
        "id": "H21Ryc9yC2rI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5gkfDkPBC8IC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Create Effective Prompts\n",
        "\n",
        "In this exercise, you will generate effective prompts for a large language model to perform specific tasks. There are several tasks for you to explore.\n",
        "\n"
      ],
      "metadata": {
        "id": "Mxr2iicoGXc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Translation\n",
        "\n",
        "**Task**: Generate a prompt that instructs the language model to translate an English text to French.\n",
        "\n",
        "1. Given text in English: \"The quick brown fox jumps over the lazy dog.\"...\n",
        "2. When the LLM translates the text to French...\n",
        "3. It should produce: \"Le rapide renard brun saute par-dessus le chien paresseux.\"\n",
        "\n",
        "_(Don't expect mini orca (GPT4All) to translate this correctly... I got \"Le renard noir a le corps de la souris.\" instead... that's because mini orca isn't trained to translate)_"
      ],
      "metadata": {
        "id": "lQ1pXWkHWLJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "  template=\"Translate the following English sentence into French: '{text}'\",\n",
        "  input_variables=[\"text\"],\n",
        ")\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "print(chain.invoke({\"text\": \"The quick brown fox jumps over the lazy dog.\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVUhK15iTCcq",
        "outputId": "1c7e814e-6f47-43d2-b7fc-f9470364dae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Le renard brun rapide saute par dessus le chien paresseux.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Summarization\n",
        "\n",
        "**Task**: Generate a prompt that instructs the language model to summarize a long article.\n",
        "\n",
        "1. Given a lengthy article\n",
        "2. When you prompt the LLM to summarize it\n",
        "3. It should produce an accurate summary\n",
        "\n",
        "Want to go deeper? [Dig into Hugging Face's tutorial on summarization and learn how you can use a \"rouge\" score to measure the quality of a model's summarial output](https://huggingface.co/learn/nlp-course/en/chapter7/5?fw=pt#summarization)"
      ],
      "metadata": {
        "id": "792ieiLhXXwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The article\n",
        "\n",
        "run this to load the article"
      ],
      "metadata": {
        "id": "XYsN9_QngbhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from: https://github.com/MostlyAdequate/mostly-adequate-guide/blob/master/ch04.md\n",
        "article = \"\"\"\n",
        "# Chapter 04: Currying\n",
        "\n",
        "## Can't Live If Livin' Is without You\n",
        "My Dad once explained how there are certain things one can live without until one acquires them. A microwave is one such thing. Smart phones, another. The older folks among us will remember a fulfilling life sans internet. For me, currying is on this list.\n",
        "\n",
        "The concept is simple: You can call a function with fewer arguments than it expects. It returns a function that takes the remaining arguments.\n",
        "\n",
        "You can choose to call it all at once or simply feed in each argument piecemeal.\n",
        "\n",
        "```js\n",
        "const add = x => y => x + y;\n",
        "const increment = add(1);\n",
        "const addTen = add(10);\n",
        "\n",
        "increment(2); // 3\n",
        "addTen(2); // 12\n",
        "```\n",
        "\n",
        "Here we've made a function `add` that takes one argument and returns a function. By calling it, the returned function remembers the first argument from then on via the closure. Calling it with both arguments all at once is a bit of a pain, however, so we can use a special helper function called `curry` to make defining and calling functions like this easier.\n",
        "\n",
        "Let's set up a few curried functions for our enjoyment. From now on, we'll summon our `curry`\n",
        "function defined in the [Appendix A - Essential Function Support](./appendix_a.md).\n",
        "\n",
        "```js\n",
        "const match = curry((what, s) => s.match(what));\n",
        "const replace = curry((what, replacement, s) => s.replace(what, replacement));\n",
        "const filter = curry((f, xs) => xs.filter(f));\n",
        "const map = curry((f, xs) => xs.map(f));\n",
        "```\n",
        "\n",
        "The pattern I've followed is a simple, but important one. I've strategically positioned the data we're operating on (String, Array) as the last argument. It will become clear as to why upon use.\n",
        "\n",
        "(The syntax `/r/g`  is a regular expression that means _match every letter 'r'_. Read [more about regular expressions](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions) if you like.)\n",
        "\n",
        "```js\n",
        "match(/r/g, 'hello world'); // [ 'r' ]\n",
        "\n",
        "const hasLetterR = match(/r/g); // x => x.match(/r/g)\n",
        "hasLetterR('hello world'); // [ 'r' ]\n",
        "hasLetterR('just j and s and t etc'); // null\n",
        "\n",
        "filter(hasLetterR, ['rock and roll', 'smooth jazz']); // ['rock and roll']\n",
        "\n",
        "const removeStringsWithoutRs = filter(hasLetterR); // xs => xs.filter(x => x.match(/r/g))\n",
        "removeStringsWithoutRs(['rock and roll', 'smooth jazz', 'drum circle']); // ['rock and roll', 'drum circle']\n",
        "\n",
        "const noVowels = replace(/[aeiou]/ig); // (r,x) => x.replace(/[aeiou]/ig, r)\n",
        "const censored = noVowels('*'); // x => x.replace(/[aeiou]/ig, '*')\n",
        "censored('Chocolate Rain'); // 'Ch*c*l*t* R**n'\n",
        "```\n",
        "\n",
        "What's demonstrated here is the ability to \"pre-load\" a function with an argument or two in order to receive a new function that remembers those arguments.\n",
        "\n",
        "I encourage you to clone the Mostly Adequate repository (`git clone\n",
        "https://github.com/MostlyAdequate/mostly-adequate-guide.git`), copy the code above and have a\n",
        "go at it in the REPL. The curry function, as well as actually anything defined in the appendixes,\n",
        "are available in the `support/index.js` module.\n",
        "\n",
        "Alternatively, have a look at a published version on `npm`:\n",
        "\n",
        "```\n",
        "npm install @mostly-adequate/support\n",
        "```\n",
        "\n",
        "## More Than a Pun / Special Sauce\n",
        "\n",
        "Currying is useful for many things. We can make new functions just by giving our base functions some arguments as seen in `hasLetterR`, `removeStringsWithoutRs`, and `censored`.\n",
        "\n",
        "We also have the ability to transform any function that works on single elements into a function that works on arrays simply by wrapping it with `map`:\n",
        "\n",
        "```js\n",
        "const getChildren = x => x.childNodes;\n",
        "const allTheChildren = map(getChildren);\n",
        "```\n",
        "\n",
        "Giving a function fewer arguments than it expects is typically called *partial application*. Partially applying a function can remove a lot of boiler plate code. Consider what the above `allTheChildren` function would be with the uncurried `map` from lodash (note the arguments are in a different order):\n",
        "\n",
        "```js\n",
        "const allTheChildren = elements => map(elements, getChildren);\n",
        "```\n",
        "\n",
        "We typically don't define functions that work on arrays, because we can just call `map(getChildren)` inline. Same with `sort`, `filter`, and other higher order functions (a *higher order function* is a function that takes or returns a function).\n",
        "\n",
        "When we spoke about *pure functions*, we said they take 1 input to 1 output. Currying does exactly this: each single argument returns a new function expecting the remaining arguments. That, old sport, is 1 input to 1 output.\n",
        "\n",
        "No matter if the output is another function - it qualifies as pure. We do allow more than one argument at a time, but this is seen as merely removing the extra `()`'s for convenience.\n",
        "\n",
        "\n",
        "## In Summary\n",
        "\n",
        "Currying is handy and I very much enjoy working with curried functions on a daily basis. It is a tool for the belt that makes functional programming less verbose and tedious.\n",
        "\n",
        "We can make new, useful functions on the fly simply by passing in a few arguments and as a bonus, we've retained the mathematical function definition despite multiple arguments.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "sdZCJZwRdcT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Your exercise\n",
        "\n",
        "Perform you work here:"
      ],
      "metadata": {
        "id": "F7rqIBGlglG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "EjrLcsWHgsRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2: Specifying the Format\n",
        "\n",
        "**Task**: Building on your solution to Task 1, improve the generated content by specifying the desired format.\n",
        "\n",
        "1. When you adjust your prompt to recommend that the summary be 3 sentences or less\n",
        "2. It should adhere to the limit and produce a shorter summary\n",
        "3. When you adjust your prompt to request bullets instead of prose\n",
        "4. It should produce a bullet list for the summary"
      ],
      "metadata": {
        "id": "dvAWjoV3vrO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "Ls4aWDNRwTg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3: Creative Writing\n",
        "\n",
        "**Task**: Create a prompt that encourages the model to generate a creative and engaging story opening or poem.\n",
        "\n",
        "1. When you provide a prompt with a scenario that you want it to write about (e.g. an event in someone's life, an interaction between people, an atmosphere in a time and place, etc.)\n",
        "2. It should produce a story or poem on the topic you requested"
      ],
      "metadata": {
        "id": "4OUbhDUOndJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "armexDxTpV5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4: Creating a Persona\n",
        "\n",
        "**Task**: Building on Task 3, give the LLM cues that influence the writing style by creating a persona.\n",
        "\n",
        "1. When you\n",
        "    - give the bot a name (e.g. You are Sean)\n",
        "    - tell it about it's style and who it is influenced by (choose author(s) whose material was likely included in the training materials for the LLM you are using)\n",
        "    - then ask it to write about what you asked for before.\n",
        "2. It should produce material that is believably associated with the persona you provided.\n",
        "3. (Optional) Include an example in the prompt. Does it improve the output or does it result in _leading the answer_ (when the example is followed too explicitly)? Examples can reduce the quality in some situations, such as creative writing, and improve the quality in other situations, such as code generation, where we desire strong pattern alignment.\n"
      ],
      "metadata": {
        "id": "eJeNdZEtxBaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "_uFM89WNypEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5: Limiting Scope\n",
        "\n",
        "**Task**: With broad and ambiguous prompts, we sometimes benefit from the LLMs creativity, but it can be difficult to dial in what we're looking for. Building on Tasks 3 or 4, narrow the scope of what you asked the LLM to generate.\n",
        "\n",
        "1. When you chainge your prompt, providing greater detail and/or constraints (e.g. if you asked the model to generate content about an event, ask it to generate content about a single person in that event, or a moment in time, rather than a long period of time or an entire event.)\n",
        "2. It should generate content that is more focused."
      ],
      "metadata": {
        "id": "v2VbfLZxwdIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "R-XMpdsczxi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 6: Code Generation\n",
        "\n",
        "**Task**: Create a prompt that instructs the model to write tests for for existing code.\n",
        "\n",
        "1. Given the following function\n",
        "\n",
        "```py\n",
        "def now_ms(get_time=time.time):\n",
        "    return round(get_time() * 1000)\n",
        "```\n",
        "\n",
        "2. When you ask the model to generate tests for the given code.\n",
        "3. It should produce code that can be used to evaluate the quality of the function\n",
        "\n",
        "_Note: does specifying the test library change the output(e.g. \"using pytest\")? What happens when you ask for both positive and negative path tests?_#@\n",
        "\n"
      ],
      "metadata": {
        "id": "2sd8y2onpiuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "Iy7G6Byw0qbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Retrieval Augmented Generation (RAG)\n",
        "\n",
        "In this exercise, you will create prompts that combine the retrieval of specific information in order to respond to a prompt using large language models.\n",
        "\n",
        "Examples are provided for a web loader, but there are [many more to choose from](https://python.langchain.com/docs/modules/data_connection/document_loaders/), as well as different [retrievers](https://python.langchain.com/docs/modules/data_connection/retrievers/), [embedders](https://python.langchain.com/docs/integrations/text_embedding), and [vector stores](https://python.langchain.com/docs/modules/data_connection/vectorstores/)."
      ],
      "metadata": {
        "id": "f-AQhuwsqDgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Q&A\n",
        "\n",
        "**Task**: Create a prompt that instructs the model to generate a concise answer to a specific question by pulling information from various markdown documents from a GitHub repository.\n",
        "\n",
        "_This example uses a [WebBaseLoader](https://python.langchain.com/docs/integrations/document_loaders/web_base) to load the content and passes it to the `load_index` function in this notebook, which will load the content, split it into chunks and then index it in a vector store so it can be used in prompt chains._\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5Vk_B4YJtmpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "def load_95729_md():\n",
        "    \"\"\"\n",
        "    Loads the Markdown documents from the heinz-95729 repository\n",
        "    \"\"\"\n",
        "    def loader(): return WebBaseLoader([\n",
        "        \"https://raw.githubusercontent.com/losandes/heinz-95729/main/Discussion-Board-Policy.md\",\n",
        "        \"https://raw.githubusercontent.com/losandes/heinz-95729/main/Project.md\",\n",
        "        \"https://raw.githubusercontent.com/losandes/heinz-95729/main/README.md\",\n",
        "    ])\n",
        "    return load_index(\n",
        "        \"./.indexes/heinz-95729-web\",\n",
        "        loader,\n",
        "    )\n",
        "\n",
        "index = load_95729_md()\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "  Answer the question based only on the following context: {context}\n",
        "  Answer in the following language: {language}\n",
        "\n",
        "  Question: {question}\"\"\")\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | index.as_retriever(),\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"language\": itemgetter(\"language\"),\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "question = \"What are the objectives for the course project?\"\n",
        "lang = \"English\"\n",
        "print(chain.invoke({\"question\": question, \"language\": lang}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5Uig1Fx3oHQ",
        "outputId": "e2aefe48-bf3b-43ea-fbb4-cfa6cb8dbb4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRACE::20246.793:: using cached index ./.indexes/heinz-95729-web\n",
            "TRACE::20246.795:: preparing embeddings\n",
            "TRACE::20247.38:: loading index\n",
            "\n",
            "\n",
            "Answer: The objectives for the course project are to gather requirements from the client, present the requirements to the client in user story format, plan the approach, agree on a branching strategy, work together as a team to implement the design, use pull-requests and code reviews to merge new code, present the solution to the class, and submit peer evaluations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Formatted Q&A\n",
        "\n",
        "**Task**: Create a prompt that instructs the model to generate a formatted answer to a specific question by pulling information from various markdown documents from a GitHub repository.\n",
        "\n",
        "1. Building on Example 1, request that the answer follows a specific format."
      ],
      "metadata": {
        "id": "nwESrkrc7FGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "  You will be provided with context and your job is to\n",
        "  answer a question using that context. Your answer\n",
        "  should be no more than 3 sentences long. Your answer\n",
        "  should use bullets. Answer in the following language: {language}\n",
        "\n",
        "  Context: {context}\n",
        "\n",
        "  Question: {question}\"\"\")\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | index.as_retriever(),\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"language\": itemgetter(\"language\"),\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "question = \"What are the objectives for the course project?\"\n",
        "lang = \"French\"\n",
        "print(chain.invoke({\"question\": question, \"language\": lang}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Dfv1NC07KBC",
        "outputId": "73253830-50d2-4890-faf7-22dc3511b85a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Réponse: \n",
            "- Réunir des exigences du client (professeur). \n",
            "- Présenter les exigences au client sous forme d'histoires utilisateur. \n",
            "- Planifier votre approche: conception par Planning Poker.\n",
            "- Convenir d'une stratégie de ramification. \n",
            "- Travailler ensemble en tant qu'équipe pour mettre en œuvre la conception. \n",
            "- Utiliser des demandes de tirage et des examens de code pour fusionner de nouveaux codes. \n",
            "- Présenter votre solution à la classe. \n",
            "- Soumettre des évaluations entre pairs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Knowledge Synthesis\n",
        "\n",
        "**Task**: Create a prompt that instructs the model to generate a concise summary of a specific topic by pulling information from various sources or documents.\n",
        "\n",
        "1. Given: find several different sources (web pages) that discuss the same or similar topics\n",
        "2. When you use the WebBaseLoader to load content from each of those sources\n",
        "3. And create a prompt that instructs the model to generate a concise summary of a specific topic shared by those sources\n",
        "4. It should produce a summary that is informed by the content you provided"
      ],
      "metadata": {
        "id": "NpjnjpDc-JXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "R-tTikvtBH1g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}